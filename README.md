# llm-benchmarking-py

A comprehensive Python library for benchmarking LLM (Large Language Model) projects with a collection of performance testing functions across multiple computational domains.

## Description

This library provides a suite of benchmarking utilities designed to test and measure the performance of Python code generated by LLMs or written manually. It includes various algorithmic challenges, control flow patterns, data structure operations, SQL queries, and string manipulations to evaluate code efficiency and correctness.

## Features

The library includes benchmarking functions across multiple categories:

### ğŸ”„ Control Flow
- **Single Loop Operations** (`control.single`): Basic iteration patterns including range summation, max finding, and modulus operations
- **Double Loop Operations** (`control.double`): Nested loop patterns for matrix operations, pair counting, and triangle summations

### ğŸ”¢ Algorithms
- **Prime Numbers** (`algorithms.primes`): Prime checking (efficient and inefficient versions), prime summation, and prime factorization
- **Sorting** (`algorithms.sort`): List sorting, Dutch flag partition, and finding max N elements

### ğŸ“Š Data Structures
- **List Operations** (`datastructures.dslist`): Comprehensive list manipulations including searching, sorting, reversing, rotating, and merging

### ğŸ”¤ String Operations
- **String Manipulation** (`strings.strops`): String reversal, palindrome checking, and other text operations

### ğŸ—„ï¸ SQL Operations
- **Database Queries** (`sql.query`): SQLite database operations including album queries, joins, and invoice reporting using the Chinook database

### ğŸ² Generators
- **Data Generation** (`generator.gen_list`): Random list and matrix generation utilities for testing

## Installation

### Prerequisites
- Python 3.8 or higher
- Poetry (for dependency management)

### Build

Install dependencies using Poetry:

```shell
poetry install
```

## Usage

### Run Main Demo

Execute the main script to see all benchmarking functions in action:

```shell
poetry run main
```

This will run demonstrations of all available benchmarking categories.

### Run Unit Tests

Run the test suite without benchmarking:

```shell
poetry run pytest --benchmark-skip tests/
```

### Run Benchmarking

Execute performance benchmarks only:

```shell
poetry run pytest --benchmark-only tests/
```

### Using the Library

You can import and use individual benchmarking modules in your code:

```python
from llm_benchmark.algorithms.primes import Primes
from llm_benchmark.control.single import SingleForLoop
from llm_benchmark.sql.query import SqlQuery

# Check if a number is prime
result = Primes.is_prime(17)  # Returns True

# Sum a range of numbers
total = SingleForLoop.sum_range(10)  # Returns 45

# Query database
found = SqlQuery.query_album('Presence')  # Returns True/False
```

## Project Structure

```
llm-benchmarking-py/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ llm_benchmark/
â”‚       â”œâ”€â”€ algorithms/       # Prime numbers, sorting algorithms
â”‚       â”œâ”€â”€ control/          # Single and double loop patterns
â”‚       â”œâ”€â”€ datastructures/   # List operations
â”‚       â”œâ”€â”€ generator/        # Random data generators
â”‚       â”œâ”€â”€ sql/              # SQL query operations
â”‚       â””â”€â”€ strings/          # String manipulation functions
â”œâ”€â”€ tests/                    # Unit and benchmark tests
â”œâ”€â”€ data/                     # Database files (e.g., chinook.db)
â”œâ”€â”€ main.py                   # Demo script
â””â”€â”€ pyproject.toml            # Project configuration
```

## Development

### Code Formatting

The project uses `black` for code formatting and `isort` for import sorting:

```shell
poetry run black .
poetry run isort .
```

### Running Tests

```shell
# All tests
poetry run pytest tests/

# With coverage
poetry run pytest --cov=llm_benchmark tests/
```

## Contributing

Contributions are welcome! Please feel free to submit pull requests or open issues for bugs, feature requests, or improvements.

## License

This project is maintained by Turin Tech AI.

## Author

Matthew Truscott (matthew.truscott@turintech.ai)
